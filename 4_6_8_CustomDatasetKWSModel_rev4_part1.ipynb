{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "## Training Your Custom Dataset Keyword Spotting Model \n",
    "## Using 4-6-8-CustomDatasetKWSModel-rev4-part1.ipynb\n",
    "\n",
    "Note to fellow TinyML students:  The original form of this Colab notebook was developed by a number of team members at Google and Harvard University.  For more information about a series of courses on TinyML, check out <a href=\"https://www.edx.org/professional-certificate/harvardx-applied-tiny-machine-learning-tinyml-for-scale\">\n",
    "Professional Certificate in\n",
    "Applied Tiny Machine Learning (TinyML) for Scale</a>.\n",
    "\n",
    "Sections of the original notebook need Tensorflow version 1.15.  Since TF 1.15 requires Python 3.6.x or 3.7.x to run and these are no longer supported as part of the Google Colab Development environment, the notebook would no longer run.\n",
    "\n",
    "To fix this problem, I have dividied the problem into two parts.  This first notebook is to be run within Google Colab and retain as much as possible of the original notebook.  The second notebook will be run locally on the User's PC.  This requires that the user has installed Anaconda within Fedora 37.\n",
    "\n",
    "More information is available at my Github repository <a href=\"https://github.com/john-mangiaracina/TinyML-CustomKeywordSpotting\">\n",
    "TinyML-CustomKeywordSpotting</a>.\n",
    "\n",
    "While perhaps not the most elegant of solutions, I have tested this and found it to run with no issues.  I hope this helps you with your studies in TinyML and I hope you find it as rewarding to use as I have developing.    \n",
    "\n",
    "Good Luck, \n",
    "\n",
    "John\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0HQ7WkLUxD4"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olCcGuF7GRVO",
    "outputId": "0a970d2a-1a9d-4f42-8c9d-a282b78863b6"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
    "!unzip v2.4.1.zip &> 0\n",
    "!mv tensorflow-2.4.1/ tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2Xne8hbyzT1"
   },
   "source": [
    "####  Let's check our version of Python used within Colab\n",
    "####  As of notebook publication on June 07, 2023, this is 3.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G43nVAIdy3uD",
    "outputId": "473ff8b1-d710-4c4c-e3c3-cb6166d56d2e"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall tensorflow -y\n",
    "#!pip install tensorflow-gpu==1.15\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZduOcH1Z4Pj"
   },
   "source": [
    "Now let's check the TensorFlow version\n",
    "As of June 07, 2023, this is 2.12.0.  For this notebook, that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pNXoZSZevOs",
    "outputId": "f2de4114-8855-410c-d11b-05967014326c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAPKHaKmy5RN"
   },
   "source": [
    "Let's now import some packages and define our path and tend to some setup issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWIJQmuTy9HU"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from google.colab import files\n",
    "!pip install ffmpeg-python &> 0\n",
    "#  We no longer need xxd in part 1 of the lab, but we will need it in part2\n",
    "#!apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QpT5x5sL0RA"
   },
   "source": [
    "### Import your Custom Dataset\n",
    "First we are going to download Pete's dataset to use as a base set of \"other words\" and \"background noise\" that you can build ontop of for your dataset. We have found that doing this will make your model work a lot better, especially if you are training it with a small amount of custom data! We STRONGLY suggest you follow this approach as it will make a large impact on your results!\n",
    "\n",
    "**Note: this *will* take a couple of minutes to run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tE7SthPtL1lY",
    "outputId": "cc36ec02-19fa-4cfc-c86e-96cacac593c3"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
    "DATASET_DIR =  'dataset/'\n",
    "!mkdir dataset\n",
    "!tar -xf speech_commands_v0.02.tar.gz -C 'dataset'\n",
    "!rm -r -f speech_commands_v0.02.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmqY3_PSMqvl"
   },
   "source": [
    "Now you'll need to upload your all of your custom audio files that you recorded using the Open Speech Recording tool (aka the ```*.ogg``` files). **Note: you can select multiple files and upload them all at once!** \n",
    "\n",
    "If you are having trouble uploading files because your internet bandwidth is too slow feel free to skip this step and you can instead pick from the words in Pete's dataset, just like -- for those of you that took Course 2 -- you did in the KWS assignment.\n",
    "\n",
    "Pete's dataset includes the following words:\n",
    "\n",
    "Options for target words are (PICK FROM THIS LIST FOR BEST RESULTS): \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", “backward”, “forward”, “follow”, “learn”,\n",
    "\n",
    "Additional words that will be used to help train the \"unknown\" label are: \"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ppOPH3NfS5W4",
    "outputId": "8790b2c8-d786-4cd3-fee3-a8f5b25db32d"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU8wwiKmTU8m"
   },
   "source": [
    "Then we can convert them into correctly trimmed WAV files and then store them in the appropriate folders in the ```DATASET_DIR```.\n",
    "We will use Pete's extract_loudest_section tool which you can find more documentation about here: https://github.com/petewarden/extract_loudest_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvD-afuDTLGS",
    "outputId": "a09b289a-bf26-446d-f846-bd6275f7f979"
   },
   "outputs": [],
   "source": [
    "# convert the ogg files to wavs\n",
    "!mkdir wavs\n",
    "!find *.ogg -print0 | xargs -0 basename -s .ogg | xargs -I {} ffmpeg -i {}.ogg -ar 16000 wavs/{}.wav\n",
    "!rm -r -f *.ogg\n",
    "\n",
    "# then use pete's tool to only extract 1 second clips from them for use with the KWS pipeline\n",
    "!mkdir trimmed_wavs\n",
    "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
    "!make -C extract_loudest_section/\n",
    "!/tmp/extract_loudest_section/gen/bin/extract_loudest_section 'wavs/*.wav' trimmed_wavs/\n",
    "!rm -r -f /wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99ydxbONTgW-",
    "outputId": "27fd0a23-db1d-4fe1-e906-1bfccf161038"
   },
   "outputs": [],
   "source": [
    "# Store them in the appropriate folders\n",
    "data_index = {}\n",
    "os.chdir('trimmed_wavs')\n",
    "search_path = os.path.join('*.wav')\n",
    "for wav_path in glob.glob(search_path):\n",
    "    original_wav_path = wav_path\n",
    "    parts = wav_path.split('_')\n",
    "    if len(parts) > 2:\n",
    "        wav_path = parts[0] + '_' + ''.join(parts[1:])\n",
    "    matches = re.search('([^/_]+)_([^/_]+)\\.wav', wav_path)\n",
    "    if not matches:\n",
    "        raise Exception('File name not in a recognized form:\"%s\"' % wav_path)\n",
    "    word = matches.group(1).lower()\n",
    "    instance = matches.group(2).lower()\n",
    "    if not word in data_index:\n",
    "      data_index[word] = {}\n",
    "    if instance in data_index[word]:\n",
    "        raise Exception('Audio instance already seen:\"%s\"' % wav_path)\n",
    "    data_index[word][instance] = original_wav_path\n",
    "\n",
    "output_dir = os.path.join('..', 'dataset')\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    pass\n",
    "for word in data_index:\n",
    "  word_dir = os.path.join(output_dir, word)\n",
    "  try:\n",
    "      os.mkdir(word_dir)\n",
    "      print('Created dir: ' + word_dir)\n",
    "  except:\n",
    "      print('Storing in existing dir: ' + word_dir)\n",
    "  for instance in data_index[word]:\n",
    "    wav_path = data_index[word][instance]\n",
    "    output_path = os.path.join(word_dir, instance + '.wav')\n",
    "    shutil.copyfile(wav_path, output_path)\n",
    "os.chdir('..')\n",
    "!rm -r -f trimmed_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koCYgUz6U8SW"
   },
   "source": [
    "Due to the revisions to the lab, this next step is no longer optional.  Please zip up your processed custom dataset.  Download it from the Files area by clicking on the three dots next to the zip file. Note: this command will take at least 3 minutes to run as the combination of your data and Pete's dataset will be relatively large!\n",
    "\n",
    "We STRONGLY suggest you use Pete's dataset for any actual training you do!\n",
    "\n",
    "Finally if you have room in your Google Drive and would like to load and store your dataset from there in the future you can mount your Google Drive in Colab [as described in this blog post](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg_AHa7VU7wG",
    "outputId": "df52b175-1876-4f8b-e418-352f8107b624"
   },
   "outputs": [],
   "source": [
    "!zip -r myKWSDataset.zip dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20F68gfuiBeB"
   },
   "source": [
    "After the zip is performed, you may see a file with a series of letters, but no file labeled myKWSDataset.zip.  Right click in the sidebar and give it a quick refresh and the zipped file should appear.\n",
    "\n",
    "Occasionally, downloads from Colab are very slow and the system will timeout before a 2G+ files can be successfully downloaded.  In those cases, try the cell below to form multiple separate zip files.  You can then right click to download all files in parallel!  (This technique saved me a lot of frustration!)\n",
    "\n",
    "Well done!  Once the file is successfully downloaded to your PC, part1 of the lab is complete.  Proceed to 4-6-8-CustomDatasetKWSModel-rev4-part2.ipynb.\n",
    "\n",
    "For more information, check out my Github repository <a href=\"https://github.com/john-mangiaracina/TinyML-CustomKeywordSpotting\">\n",
    "TinyML-CustomKeywordSpotting</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaVtYN4nlCft"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ludfxbNIaegy",
    "outputId": "5a4cc11b-ed04-4caf-b08b-d174aab887f6"
   },
   "outputs": [],
   "source": [
    "#  forms multiple zip files for || download\n",
    "#  Use if single file download is to slow, times out\n",
    "\n",
    "#!zip -s 500M new.zip myKWSDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oh0VvWdxitw2",
    "outputId": "9f437202-fbdc-4995-bdf8-00c47533dd08"
   },
   "outputs": [],
   "source": [
    "#  Optional\n",
    "\n",
    "#  I experimented with this code so Colab wouldn't prematurely time me out\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"I'm here\")\n",
    "\n",
    "for num in range(1, 200):\n",
    "  time.sleep(60);\n",
    "  print(\"Colab, I'm still here\", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFg1DqlmmKSA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
