{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "## Training Your Custom Dataset Keyword Spotting Model \n",
    "## Using 4-6-8-CustomDatasetKWSModel-rev4-part2.ipynb\n",
    "\n",
    "*Important  Please Read This!*\n",
    "\n",
    "This notebook is the second of the custom dataset KWS Model notebooks.  While part 1 was run in Colab, the second  will be run locally on the User's PC.  The assumption is that the User is using Fedora and has Anaconda installed.  For more information, check out the project repository <a href=\"https://github.com/john-mangiaracina/TinyML-CustomKeywordSpotting/tree/main\">here</a>.  Note:  any RHEL-based distro, such as Fedora, CentOS, etc. should work.  Only Fedora 37 and Fedora 38 have been tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "Clone the TensorFlow Github Repository, which contains the relevant code required to run this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
    "!unzip v2.4.1.zip &> 0\n",
    "!mv tensorflow-2.4.1/ tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G43nVAIdy3uD",
    "outputId": "6a03e29b-cd5d-4268-8654-f90466ab767e"
   },
   "outputs": [],
   "source": [
    "#  Let's check our version of Python used within conda\n",
    "#  This should say 3.7.16.  If it doesn't, you are not in the right environment\n",
    "# and need to go back to the readme in the repo.\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAPKHaKmy5RN"
   },
   "source": [
    "####   This should show an error.  Tensorflow should not be pre-installed.  If it is, return to the readme in the repo.\n",
    "\n",
    "####   Let's now install Tensorflow 1.15.0  This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install tensorflow=1.15 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Important!\n",
    "\n",
    "#### Restart the kernel, either from the dropdown menu or from the half circle with an arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now let's check the TensorFlow version.  It should show 1.15.0\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "#print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check whether you are using a 32 or 64 bit system.  TF needs 64 bits\n",
    "\n",
    "import struct\n",
    "print(struct.calcsize(\"P\") * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Now let's install the google colab package.  Use pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge google-colab \n",
    "!pip install google-colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now let's check the TensorFlow version.  It should show 1.15.0\n",
    "\n",
    "import tensorflow as tf\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  verify google colab packages installed\n",
    "\n",
    "!pip show google-colab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Important!   Before the next code is run, make sure you have unzip your custom dataset download.  Within the unzipped folder, there will be a folder that says 'dataset'.  Move this folder into the same directory as you have this jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we have to edit some files within the Python 3.7 lib within your virtual dev env\n",
    "#  This assumes that yopu have followed the default suggestions and installed anaconda in the default dir \n",
    "\n",
    "!sed -i 's/from traitlets import */import traitlets /g' ~/anaconda3/envs/myenv37/lib/python3.7/site-packages/IPython/utils/traitlets.py \n",
    "!sed -i 's/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/g' ~/anaconda3/envs/myenv37/lib/python3.7/site-packages/google/colab/data_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWIJQmuTy9HU"
   },
   "outputs": [],
   "source": [
    "# Note:The install assumes that the files downloaded for part1 and this notebook are in the same directory.\n",
    "\n",
    "import sys\n",
    "\n",
    "# We add this path so we can import the speech processing modules.\n",
    "sys.path.append(\"tensorflow/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python &> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaVtYN4nlCft"
   },
   "source": [
    "### Configure Your Model!\n",
    "Ok now that your custom dataset is all ready to go we'll need to select your keywords and model settings with which to train!\n",
    "\n",
    "```WANTED_WORDS``` = A comma-delimited string of the words you want to train for (e.g., \"yes,no\"). \n",
    "\n",
    "**Make sure to input the keywords you collected!  Also, make sure to have no spaces within the string.  The string must only contain letters and commas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ludfxbNIaegy"
   },
   "outputs": [],
   "source": [
    "WANTED_WORDS = \"pizza,coke,bread\"   #  do not have something like \"pizza,  coke,   bread\"  it won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8VBOW8Ob7ys"
   },
   "source": [
    "The number of training steps and learning rates can be specified as comma-separated strings to define the amount/rate at each stage. For example, ```TRAINING_STEPS=\"12000,3000\"``` and ```LEARNING_RATE=\"0.001,0.0001\"``` will run 12,000 training steps with a rate of 0.001 followed by 3,000 final steps with a learning rate of 0.0001. These are good default values to work off of when you choose your values as the course staff has gotten this to work well with those values in the past!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1eWOvdfcPlo"
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = \"30000,4000\"   #  This will take a while, setup before dinner and watching some ball\n",
    "LEARNING_RATE = \"0.001,0.0001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q36xx9vcQVp"
   },
   "source": [
    "We suggest you leave the ```MODEL_ARCHITECTURE``` as tiny_conv the first time but if you would like to do this again and explore additional models some options are: ```single_fc, conv, low_latency_conv, low_latency_svdf, tiny_embedding_conv```. **Do remember if you switch the model type you may need to update the C++ code to include the ```tflite::AllOpsResolver``` to make sure you have all of the neccessary ops!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3Y1phllccMn"
   },
   "outputs": [],
   "source": [
    "MODEL_ARCHITECTURE = 'tiny_conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhj3jQr0cfCt"
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCgeOpvY9pAi"
   },
   "source": [
    "**We suggest that you do not modify** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd1iM1o2ymvA"
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "# Constants used during training only\n",
    "VERBOSITY = 'DEBUG'\n",
    "EVAL_STEP_INTERVAL = '1000'\n",
    "SAVE_STEP_INTERVAL = '1000'\n",
    "\n",
    "# Constants for training directories and filepaths\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
    "\n",
    "# Constants for inference directories and filepaths\n",
    "import os\n",
    "MODELS_DIR = 'models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = os.path.join(MODELS_DIR, 'KWS_custom.pb')\n",
    "MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom.tflite')\n",
    "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom_float.tflite')\n",
    "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'KWS_custom.cc')\n",
    "SAVED_MODEL = os.path.join(MODELS_DIR, 'KWS_custom_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiKisyFN4R6W"
   },
   "source": [
    "**Be careful if you modify** the following constants as they will have downstream effects on the C++ code which you will then have to change. This mainly relate to hyperparaemeters for quantization and preprocessing. The first time you train a custom model **we suggest you do not modify these as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbdtOiLV4SMD"
   },
   "outputs": [],
   "source": [
    "# Constants which are shared during training and inference\n",
    "PREPROCESS = 'micro'\n",
    "WINDOW_STRIDE = 20\n",
    "\n",
    "# Constants for Quantization\n",
    "QUANT_INPUT_MIN = 0.0\n",
    "QUANT_INPUT_MAX = 26.0\n",
    "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
    "\n",
    "# Constants for audio process during Quantization and Evaluation\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.1\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "# Use the custom local dataset and set the tes/val/train split\n",
    "DATA_URL = ''\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's print out some things as part of debugging\n",
    "#  I have left this because others may find it helpful\n",
    "\n",
    "print(SILENT_PERCENTAGE)\n",
    "print(UNKNOWN_PERCENTAGE)\n",
    "print(WINDOW_STRIDE)\n",
    "print(EVAL_STEP_INTERVAL)\n",
    "print(SAVE_STEP_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  part of debugging process to try to figure out what the blazes is going on\n",
    "\n",
    "SILENT_PERCENTAGE = float(SILENT_PERCENTAGE)\n",
    "UNKNOWN_PERCENTAGE = float(UNKNOWN_PERCENTAGE)\n",
    "WINDOW_STRIDE = float(WINDOW_STRIDE)\n",
    "\n",
    "print(SILENT_PERCENTAGE)\n",
    "print(UNKNOWN_PERCENTAGE)\n",
    "print(WINDOW_STRIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UczQKtqLi7OJ"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "### Load in TensorBoard to visulaize the training process.\n",
    "\n",
    "As training progresses you should see the training status show up in the Tensorboard area. If this works it is very helpful for analyzing your training progress. Unfortunately, the staff has found that it sometimes doesn't start showing data for a while (~15 minutes) and sometimes doesn't show data until training completes (and instead shows ```No dashboards are active for the current data set```.). If it is working and then stops updating look to the top of the cell and click reconnect.\n",
    "\n",
    "--->>> Tensorboard works great until it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR='dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozkJqgSkU_qI"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gweRC9eWVdg2"
   },
   "source": [
    "### Launch Training\n",
    "\n",
    "--->>>  Note:  We will *not* be using a GPU.  Specific nVidia solutions are required for Anaconda.  I burned up a lot of cycles figuring out my GPU is not supported.  Just fuhgeddaboudit!\n",
    "\n",
    "The original writeup stated 10 hours on Colab with no GPU, 2 hours with GPU.  I believe them, but that is with the Colab Dev Env.\n",
    "\n",
    "When I ran this on a ThinkPad, the training portion took around 1 hour and 5 minutes to run 15,000 training cycles.  Since it is on your own system, you don't need to worry about being timed out.  Do something else while is runs. \n",
    "\n",
    "Most likely, your computer will complete this task much faster then mine.\n",
    "\n",
    "If you follow my settings, you should be OK.  Of course, feel free to experiment! That's where things get really fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to get more information on the training script you can find the source code for the script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py). In short it sets up the optimizer and preprocessor based on all of the flags we pass in!\n",
    "\n",
    "*Note:  the above paragraph should really be required.  It is worth your time to check out the default values.*\n",
    "\n",
    "Finally, by setting the ```VERBOSITY = 'DEBUG'``` above be aware that the training cell will print A LOT of information. Specifically you will get the accuracy and loss at each step as well as a confusion matrix every 1000 steps. We hope that is helpful in case TensorBoard fails to work. If you would like to run with less printouts you can change the setting to ```WARN``` or ```FATAL```. You will find this in the \"Configure Your Model!\" section.\n",
    "\n",
    "*Note:  Keeping the setting at DEBUG jacks up the  uP cycles devoted to i/o but is extremely helpful.  I suggest keeping it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZw3VNlnla-J"
   },
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "  --data_dir={DATASET_DIR} \\\n",
    "  --data_url='' \\\n",
    "  --wanted_words={WANTED_WORDS} \\\n",
    "  --silence_percentage=20.0 \\\n",
    "  --unknown_percentage=20.0 \\\n",
    "  --preprocess='micro' \\\n",
    "  --window_stride=20.0 \\\n",
    "  --model_architecture={MODEL_ARCHITECTURE} \\\n",
    "  --how_many_training_steps={TRAINING_STEPS} \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --train_dir={TRAIN_DIR} \\\n",
    "  --summaries_dir={LOGS_DIR} \\\n",
    "  --verbosity='DEBUG' \\\n",
    "  --eval_step_interval=1000 \\\n",
    "  --save_step_interval=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjBn-NapWsho"
   },
   "source": [
    "# Generating your Model\n",
    "Just like with the pre-trained model we will now take the final checkpoint and convert it into a quantized TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQUJLrdS-ftl"
   },
   "source": [
    "### Generate a TensorFlow Model for Inference\n",
    "\n",
    "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyc3_eLh9sAg"
   },
   "outputs": [],
   "source": [
    "!rm -rf {SAVED_MODEL}\n",
    "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--wanted_words=$WANTED_WORDS \\\n",
    "--window_stride_ms=$WINDOW_STRIDE \\\n",
    "--preprocess=$PREPROCESS \\\n",
    "--model_architecture=$MODEL_ARCHITECTURE \\\n",
    "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
    "--save_format=saved_model \\\n",
    "--output_file={SAVED_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DBGDxVI-nKG"
   },
   "source": [
    "### Generate a TensorFlow Lite Model\n",
    "\n",
    "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices. The following cell will also print the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNQdAplJV1fz"
   },
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,\n",
    "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-rvCCM2G58N"
   },
   "source": [
    "**Note: if the below cell fails it might be because you do not have enough data to have 100 recordings in the representative dataset!** If this happens you will see an error that says something like ```ValueError: cannot reshape array of size 0 into shape (1,1960)```. To help you fix this we have added a ```print(i)``` into the loop. As such, all you have to do is change the ```REP_DATA_SIZE``` variable to be equal to the last integer value printed out by the loop and then re-run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  used for debugging\n",
    "\n",
    "print(SILENT_PERCENTAGE)\n",
    "print(PREPROCESS)\n",
    "print(WINDOW_STRIDE)\n",
    "print(VERBOSITY)\n",
    "print(EVAL_STEP_INTERVAL)\n",
    "print(SAVE_STEP_INTERVAL)\n",
    "print(DATA_URL)\n",
    "print(BACKGROUND_FREQUENCY)\n",
    "print(BACKGROUND_VOLUME_RANGE)\n",
    "print(TIME_SHIFT_MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [],
   "source": [
    "REP_DATA_SIZE = 23  #   <---  Don't be surprised if you have to change this value\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
    "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  float_tflite_model = float_converter.convert()\n",
    "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.inference_input_type = tf.lite.constants.INT8\n",
    "  # converter.inference_input_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x   \n",
    "  converter.inference_output_type = tf.lite.constants.INT8\n",
    "  # converter.inference_output_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
    "  def representative_dataset_gen():\n",
    "    for i in range(REP_DATA_SIZE):\n",
    "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                         BACKGROUND_FREQUENCY, \n",
    "                                         BACKGROUND_VOLUME_RANGE,\n",
    "                                         TIME_SHIFT_MS,\n",
    "                                         'testing',\n",
    "                                         sess)\n",
    "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "      print(i)\n",
    "      yield [flattened_data]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  tflite_model = converter.convert()\n",
    "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeLiDZTbLkzv"
   },
   "source": [
    "### Testing the accuracy after Quantization\n",
    "\n",
    "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQsEteKRLryJ"
   },
   "outputs": [],
   "source": [
    "# Helper function to run inference\n",
    "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
    "  #\n",
    "  # Load test data\n",
    "  #\n",
    "  np.random.seed(0) # set random seed for reproducible test results.\n",
    "  with tf.Session() as sess:\n",
    "    # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
    "    test_data, test_labels = audio_processor.get_data(\n",
    "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "        TIME_SHIFT_MS, 'testing', sess)\n",
    "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "  #\n",
    "  # Initialize the interpreter\n",
    "  #\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  \n",
    "  #\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  #\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  #\n",
    "  # Evaluate the predictions\n",
    "  #\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    top_prediction = output.argmax()\n",
    "    correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-pD52Na6jRa"
   },
   "outputs": [],
   "source": [
    "# Compute float model accuracy\n",
    "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf6CQuooWQy5"
   },
   "source": [
    "### Generate a TensorFlow Lite for Microcontrollers Model\n",
    "To convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on Arduino we simply need to use the ```xxd``` tool to convert the ```.tflite``` file into a ```.cc``` file (just like we did in the pervious section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwStW8XCWXXX"
   },
   "outputs": [],
   "source": [
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-AhT48A17Eo"
   },
   "source": [
    "That's it! You've successfully converted your TensorFlow Lite model into a TensorFlow Lite for Microcontrollers model! Run the cell below to print out its contents which we'll need for our next step, deploying the model using the Arudino IDE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79XIezycHhp6"
   },
   "outputs": [],
   "source": [
    "!cat {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQxu8s15HbfJ"
   },
   "source": [
    "To download your model for use at a later date:\n",
    "\n",
    "1. On the left of the UI click on the folder icon\n",
    "2. Click on the three dots to the right of the ```.cc``` file you just generated and select \"download.\" The file can be found at ```models/{MODEL_TFLITE_MICRO}``` which by default is ```models/KWS_custom.cc```\n",
    "\n",
    "Next *you will* deploy that model using the Arduino IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
